{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "melTU-rvVbNT"
   },
   "source": [
    "# SparkSQL Partitioning Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D93u7gEUVbNg"
   },
   "source": [
    "First of all load the flights data from a JSON file into a DataFrame. Then we will select the columns that we will be use for the flight edge DataFrame. The required column names are id, src, and dst, so we rename those columns in the select statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wBG5LbWXVbNh",
    "outputId": "60b2b6fd-f626-4e7d-c8a5-53f88def14ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- arrdelay: double (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- crsarrtime: long (nullable = true)\n",
      " |-- crsdephour: long (nullable = true)\n",
      " |-- crsdeptime: long (nullable = true)\n",
      " |-- crselapsedtime: double (nullable = true)\n",
      " |-- depdelay: double (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- dist: double (nullable = true)\n",
      " |-- dofW: long (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightdata = spark.read.option(\"inferSchema\", \"false\").json(\"data/flightsdata.json\");\n",
    "\n",
    "flightdata.printSchema();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7m1khP63VbNj",
    "outputId": "69f3a208-d41b-4bbd-d25c-39b874b4b3a6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- arrdelay: double (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- crsarrtime: long (nullable = true)\n",
      " |-- crsdephour: long (nullable = true)\n",
      " |-- crsdeptime: long (nullable = true)\n",
      " |-- crselapsedtime: double (nullable = true)\n",
      " |-- delay: double (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- dist: double (nullable = true)\n",
      " |-- dofW: long (nullable = true)\n",
      " |-- src: string (nullable = true)\n",
      "\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+-----+---+------+----+---+\n",
      "|                  id|arrdelay|carrier|crsarrtime|crsdephour|crsdeptime|crselapsedtime|delay|dst|  dist|dofW|src|\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+-----+---+------+----+---+\n",
      "|AA_2017-01-01_ATL...|     0.0|     AA|      1912|        17|      1700|         132.0|  0.0|LGA| 762.0|   7|ATL|\n",
      "|AA_2017-01-01_LGA...|     0.0|     AA|      1620|        13|      1343|         157.0|  0.0|ATL| 762.0|   7|LGA|\n",
      "|AA_2017-01-01_MIA...|    10.0|     AA|      1137|         9|       939|         118.0|  0.0|ATL| 594.0|   7|MIA|\n",
      "|AA_2017-01-01_ORD...|     0.0|     AA|        26|        20|      2020|         186.0|  0.0|MIA|1197.0|   7|ORD|\n",
      "|AA_2017-01-01_LGA...|     0.0|     AA|      1017|         7|       700|         197.0|  0.0|MIA|1096.0|   7|LGA|\n",
      "|AA_2017-01-01_ORD...|     0.0|     AA|      1527|        13|      1345|         162.0|  0.0|DEN| 888.0|   7|ORD|\n",
      "|AA_2017-01-01_DEN...|     0.0|     AA|      1649|        11|      1100|         229.0|  0.0|MIA|1709.0|   7|DEN|\n",
      "|AA_2017-01-01_MIA...|     0.0|     AA|       949|         7|       720|         269.0|  7.0|DEN|1709.0|   7|MIA|\n",
      "|AA_2017-01-01_DEN...|     0.0|     AA|      1600|        12|      1235|         145.0|  0.0|ORD| 888.0|   7|DEN|\n",
      "|AA_2017-01-01_ORD...|     3.0|     AA|      1145|        10|      1005|         160.0|  5.0|DEN| 888.0|   7|ORD|\n",
      "|AA_2017-01-01_IAH...|     0.0|     AA|      1025|         7|       701|         144.0|  9.0|MIA| 964.0|   7|IAH|\n",
      "|AA_2017-01-01_MIA...|    63.0|     AA|      2238|        20|      2045|         173.0| 80.0|IAH| 964.0|   7|MIA|\n",
      "|AA_2017-01-01_MIA...|     0.0|     AA|      2325|        20|      2025|         180.0|  0.0|LGA|1096.0|   7|MIA|\n",
      "|AA_2017-01-01_DEN...|     7.0|     AA|       550|        24|      2359|         231.0| 23.0|MIA|1709.0|   7|DEN|\n",
      "|AA_2017-01-01_ORD...|     0.0|     AA|      1818|        15|      1508|         130.0|  0.0|BOS| 867.0|   7|ORD|\n",
      "|AA_2017-01-01_MIA...|    73.0|     AA|      2049|        18|      1756|         173.0| 85.0|LGA|1096.0|   7|MIA|\n",
      "|AA_2017-01-01_MIA...|     0.0|     AA|      1231|        10|      1005|         206.0|  4.0|ORD|1197.0|   7|MIA|\n",
      "|AA_2017-01-01_MIA...|    43.0|     AA|      1846|        16|      1621|         265.0| 43.0|DEN|1709.0|   7|MIA|\n",
      "|DL_2017-01-01_MIA...|     0.0|     DL|      2105|        19|      1900|         125.0|  4.0|ATL| 594.0|   7|MIA|\n",
      "|DL_2017-01-01_MIA...|    35.0|     DL|      1823|        16|      1618|         125.0| 29.0|ATL| 594.0|   7|MIA|\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+-----+---+------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights = flightdata.withColumnRenamed(\"_id\", \"id\").withColumnRenamed(\"origin\", \"src\").withColumnRenamed(\"dest\", \"dst\").withColumnRenamed(\"depdelay\", \"delay\");\n",
    "\n",
    "flights.printSchema();\n",
    "flights.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BmoJMioVbNk"
   },
   "source": [
    "## Parquet Partitioning\n",
    "Spark table's partitioning optimizes reads by storing files into hierarchy of directories based on columns using which they are partitioned.\n",
    "Here is the code to persist a flights DataFrame as a table consisting of Parquet files partitioned by the src column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fJwP0T6WVbNk"
   },
   "outputs": [],
   "source": [
    "flights.write.mode(\"overwrite\").format(\"parquet\").partitionBy(\"src\").option(\"path\", \"data/flights\").saveAsTable(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30WotvG1VbNl"
   },
   "source": [
    "Below is the resulting directory structure as shown by a Hadoop list files command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "niE-jF17VbNl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Found 10 items\n",
      "-rw-r--r--   1 training supergroup          0 2021-03-08 17:17 data/flights/_SUCCESS\n",
      "drwxr-xr-x   - training supergroup          0 2021-03-08 17:17 data/flights/src=ATL\n",
      "drwxr-xr-x   - training supergroup          0 2021-03-08 17:17 data/flights/src=BOS\n",
      "drwxr-xr-x   - training supergroup          0 2021-03-08 17:17 data/flights/src=DEN\n",
      "drwxr-xr-x   - training supergroup          0 2021-03-08 17:17 data/flights/src=EWR\n",
      "drwxr-xr-x   - training supergroup          0 2021-03-08 17:17 data/flights/src=IAH\n",
      "drwxr-xr-x   - training supergroup          0 2021-03-08 17:17 data/flights/src=LGA\n",
      "drwxr-xr-x   - training supergroup          0 2021-03-08 17:17 data/flights/src=MIA\n",
      "drwxr-xr-x   - training supergroup          0 2021-03-08 17:17 data/flights/src=ORD\n",
      "drwxr-xr-x   - training supergroup          0 2021-03-08 17:17 data/flights/src=SFO\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data/flights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5UZBzRJVbNl"
   },
   "source": [
    "Below, we see that the src=ATL subdirectory contains Parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nPBiG8UrVbNm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Found 2 items\n",
      "-rw-r--r--   1 training supergroup      59191 2021-03-08 17:17 data/flights/src=ATL/part-00000-5910ff2c-0ef5-4710-bd1b-24e7442910b5.c000.snappy.parquet\n",
      "-rw-r--r--   1 training supergroup      25909 2021-03-08 17:17 data/flights/src=ATL/part-00001-5910ff2c-0ef5-4710-bd1b-24e7442910b5.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data/flights/src=ATL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MMM1EipVbNm"
   },
   "source": [
    "## Partition Pruning and Predicate Pushdown\n",
    "**Partition pruning** \n",
    ">It is a performance optimization that limits the number of files and partitions that Spark reads when querying.<br> \n",
    ">After partitioning, queries which certainly matches the filter criteria partitions for improving the performance by allowing Spark to only read a subset of the directories and files.<br> \n",
    ">When partition filters are present, the catalyst optimizer pushes down the partition filters.<br> \n",
    ">The scan reads only the directories that match the partition filters, which actually reduces the disk I/O.<br> \n",
    ">For example, the following query reads only the files in the src=DEN partition directory in order to query the average departure delay for flights originating from Denver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vEIWWYCnVbNn",
    "outputId": "1232050d-bb62-414d-ef1e-08a9ba4e2db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "|src|dst|        avg(delay)|\n",
      "+---+---+------------------+\n",
      "|DEN|SFO| 51.55853658536585|\n",
      "|DEN|EWR|47.881147540983605|\n",
      "|DEN|LGA| 41.92549019607843|\n",
      "|DEN|ORD| 40.98522167487685|\n",
      "|DEN|ATL| 36.64888888888889|\n",
      "|DEN|IAH|28.633333333333333|\n",
      "|DEN|BOS|24.432989690721648|\n",
      "|DEN|MIA|22.155172413793103|\n",
      "+---+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "flights.filter(\"src = 'DEN' and delay > 1\").groupBy(\"src\", \"dst\").avg(\"delay\").sort(desc(\"avg(delay)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyR9COW_VbNn"
   },
   "source": [
    "One can see the physical plan for a DataFrame query in the Spark web UI SQL tab or by calling the explain method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "F-kGl_WeVbNn",
    "outputId": "9214487f-4541-4a27-fa80-25255026827a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['avg(delay) DESC NULLS LAST], true\n",
      "+- Aggregate [src#44, dst#57], [src#44, dst#57, avg(delay#70) AS avg(delay)#234]\n",
      "   +- Filter ((src#44 = DEN) AND (delay#70 > cast(1 as double)))\n",
      "      +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14 AS delay#70, dst#57, dist#16, dofW#17L, src#44]\n",
      "         +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15 AS dst#57, dist#16, dofW#17L, src#44]\n",
      "            +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18 AS src#44]\n",
      "               +- Project [_id#7 AS id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18]\n",
      "                  +- RelationV2[_id#7, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18] json hdfs://training.io:8020/user/training/data/flightsdata.json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "src: string, dst: string, avg(delay): double\n",
      "Sort [avg(delay)#234 DESC NULLS LAST], true\n",
      "+- Aggregate [src#44, dst#57], [src#44, dst#57, avg(delay#70) AS avg(delay)#234]\n",
      "   +- Filter ((src#44 = DEN) AND (delay#70 > cast(1 as double)))\n",
      "      +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14 AS delay#70, dst#57, dist#16, dofW#17L, src#44]\n",
      "         +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15 AS dst#57, dist#16, dofW#17L, src#44]\n",
      "            +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18 AS src#44]\n",
      "               +- Project [_id#7 AS id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18]\n",
      "                  +- RelationV2[_id#7, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18] json hdfs://training.io:8020/user/training/data/flightsdata.json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [avg(delay)#234 DESC NULLS LAST], true\n",
      "+- Aggregate [src#44, dst#57], [src#44, dst#57, avg(delay#70) AS avg(delay)#234]\n",
      "   +- Project [depdelay#14 AS delay#70, dest#15 AS dst#57, origin#18 AS src#44]\n",
      "      +- Filter (((isnotnull(origin#18) AND isnotnull(depdelay#14)) AND (origin#18 = DEN)) AND (depdelay#14 > 1.0))\n",
      "         +- RelationV2[depdelay#14, dest#15, origin#18] json hdfs://training.io:8020/user/training/data/flightsdata.json\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [avg(delay)#234 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(avg(delay)#234 DESC NULLS LAST, 200), true, [id=#95]\n",
      "   +- *(2) HashAggregate(keys=[src#44, dst#57], functions=[avg(delay#70)], output=[src#44, dst#57, avg(delay)#234])\n",
      "      +- Exchange hashpartitioning(src#44, dst#57, 200), true, [id=#91]\n",
      "         +- *(1) HashAggregate(keys=[src#44, dst#57], functions=[partial_avg(delay#70)], output=[src#44, dst#57, sum#243, count#244L])\n",
      "            +- *(1) Project [depdelay#14 AS delay#70, dest#15 AS dst#57, origin#18 AS src#44]\n",
      "               +- *(1) Filter (((isnotnull(origin#18) AND isnotnull(depdelay#14)) AND (origin#18 = DEN)) AND (depdelay#14 > 1.0))\n",
      "                  +- BatchScan[depdelay#14, dest#15, origin#18] JsonScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/flightsdata.json], ReadSchema: struct<depdelay:double,dest:string,origin:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.filter(\"src = 'DEN' and delay > 1\").groupBy(\"src\", \"dst\").avg(\"delay\").sort(desc(\"avg(delay)\")).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPJMEA6DVbNo"
   },
   "source": [
    "The physical plan is read from the bottom up, whereas the DAG is read from the top down. Note: the Exchange means a shuffle occurred between stages.\n",
    "\n",
    "<img src=\"https://i.ibb.co/QNKMdCJ/1.jpg\" alt=\"1\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pAcS3AdVbNo"
   },
   "source": [
    "## Partitioning Tips\n",
    "The partition columns must be used frequently in queries for filtering and must have a small range of values with corresponding data for distributing the files in the directories. \n",
    "If you want to avoid too many small files, which actually make scans less efficient with parallelism. You also should avoid having too few large files, which can hurt parallelism.\n",
    "\n",
    "## Coalesce and Repartition\n",
    "\n",
    "Before or when writing a DataFrame, you can use dataframe.coalesce(N) to reduce the number of partitions in a DataFrame, without shuffling, or df.repartition(N) to reorder and either increase or decrease the number of partitions with shuffling data across the network to achieve even load balancing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3XdIvRUgVbNp"
   },
   "outputs": [],
   "source": [
    "flights.repartition(13).write.mode(\"overwrite\").format(\"parquet\").partitionBy(\"src\").option(\"path\", \"data/flights\").saveAsTable(\"flights\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSCmK9evVbNp"
   },
   "source": [
    "## Bucketing\n",
    "* Bucketing is one of the data organization technique which groups data with the same bucket value within a fixed number of “buckets.” \n",
    "* It can improve performance in wide transformations and joins by avoiding “shuffles.” \n",
    "* Bucketing is similar to partitioning, but in partitions we generally creates a directory for each partitions, whereas in bucketing data is distributed across a fixed number of buckets by a hash on the bucket value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YQUPRRGAVbNq"
   },
   "outputs": [],
   "source": [
    "flights.write.format(\"parquet\").sortBy(\"id\").partitionBy(\"src\").bucketBy(4,\"dst\",\"carrier\").option(\"path\",\"data/flightsbkdc\").saveAsTable(\"flightsbkdc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuZwLq13VbNq"
   },
   "source": [
    "The resulting directory structure is the same as before, with the files in the src directories bucketed by dst and carrier. The code below computes statistics on the table, which can then be used by the Catalyst optimizer. After that, the partitioned and bucketed table is read into a new DataFrame df2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qDOKYXaNVbNq"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"ANALYZE TABLE flightsbkdc COMPUTE STATISTICS\")\n",
    "df2  = spark.table(\"flightsbkdc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43eZs5HkVbNr"
   },
   "source": [
    "Next, let’s look at the optimizations for the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0lchVcZWVbNr",
    "outputId": "2d1d39c8-9620-45f2-b971-ad27e546c642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+------------------+\n",
      "|src|dst|carrier|        avg(delay)|\n",
      "+---+---+-------+------------------+\n",
      "|DEN|LGA|     DL| 82.03333333333333|\n",
      "|DEN|SFO|     UA|57.437275985663085|\n",
      "|DEN|EWR|     UA|57.027027027027025|\n",
      "|DEN|ATL|     DL| 42.94736842105263|\n",
      "|DEN|ORD|     UA| 41.07801418439716|\n",
      "|DEN|ORD|     AA|40.774193548387096|\n",
      "|DEN|SFO|     WN|39.038167938931295|\n",
      "|DEN|EWR|     WN|          33.78125|\n",
      "|DEN|LGA|     UA| 32.66386554621849|\n",
      "|DEN|MIA|     UA|              31.0|\n",
      "|DEN|IAH|     UA|28.633333333333333|\n",
      "|DEN|ATL|     WN|  28.0561797752809|\n",
      "|DEN|BOS|     UA|26.966101694915253|\n",
      "|DEN|LGA|     WN|24.763157894736842|\n",
      "|DEN|BOS|     WN|              20.5|\n",
      "|DEN|MIA|     AA| 16.34285714285714|\n",
      "|DEN|ATL|     UA|12.333333333333334|\n",
      "+---+---+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(\"src = 'DEN' and delay > 1\").groupBy(\"src\", \"dst\",\"carrier\").avg(\"delay\").sort(desc(\"avg(delay)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfMAh5z9VbNr"
   },
   "source": [
    "Here, we can see partition filter and filter pushdown, but we can also see that there is no “Exchange” like before bucketing, which means there was no shuffle to aggregate by src, dst, and carrier.\n",
    "\n",
    "<img src=\"https://i.ibb.co/fH3hfSd/2.jpg\" alt=\"2\" border=\"0\">\n",
    "\n",
    "In the DAG(Direct Acyclic Graph), we can see that there is no exchange shuffle, and we can also see “Whole-Stage Java Code Generation,” which actually optimizes CPU usage, by generating a single optimized function in bytecode.\n",
    "\n",
    "<img src=\"https://i.ibb.co/GPzXNwC/3.jpg\" alt=\"3\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDFDZnyLVbNu"
   },
   "source": [
    "## Projection and Filter Pushdown\n",
    "* Below, we see the physical plan for a DataFrame query, with projection and filter pushdown. \n",
    "* This means that the scanning of the src, dst, and depdelay columns and the filter on the depdelay column are pushed down into Source.\n",
    "* Meaning that the scanning and filtering will take place in Source before than returning the data to Spark. \n",
    "* Projection pushdown actually minimizes the data transfer between the Source and the Spark engine by removing the unnecessary fields from table scans. \n",
    "* It is especially beneficial when a table contains many columns. \n",
    "* Filter pushdown actually improves performance by reducing the amount of the data passed between the Source and the Spark engine when filtering data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5nd5AVJlVbNv",
    "outputId": "c7bc9e9a-2219-48cc-ab08-55efb1325763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['avg(delay) DESC NULLS LAST], true\n",
      "+- Aggregate [src#44, dst#57], [src#44, dst#57, avg(delay#70) AS avg(delay)#440]\n",
      "   +- Filter ((src#44 = ATL) AND (delay#70 > cast(1 as double)))\n",
      "      +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14 AS delay#70, dst#57, dist#16, dofW#17L, src#44]\n",
      "         +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15 AS dst#57, dist#16, dofW#17L, src#44]\n",
      "            +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18 AS src#44]\n",
      "               +- Project [_id#7 AS id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18]\n",
      "                  +- RelationV2[_id#7, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18] json hdfs://training.io:8020/user/training/data/flightsdata.json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "src: string, dst: string, avg(delay): double\n",
      "Sort [avg(delay)#440 DESC NULLS LAST], true\n",
      "+- Aggregate [src#44, dst#57], [src#44, dst#57, avg(delay#70) AS avg(delay)#440]\n",
      "   +- Filter ((src#44 = ATL) AND (delay#70 > cast(1 as double)))\n",
      "      +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14 AS delay#70, dst#57, dist#16, dofW#17L, src#44]\n",
      "         +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15 AS dst#57, dist#16, dofW#17L, src#44]\n",
      "            +- Project [id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18 AS src#44]\n",
      "               +- Project [_id#7 AS id#31, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18]\n",
      "                  +- RelationV2[_id#7, arrdelay#8, carrier#9, crsarrtime#10L, crsdephour#11L, crsdeptime#12L, crselapsedtime#13, depdelay#14, dest#15, dist#16, dofW#17L, origin#18] json hdfs://training.io:8020/user/training/data/flightsdata.json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [avg(delay)#440 DESC NULLS LAST], true\n",
      "+- Aggregate [src#44, dst#57], [src#44, dst#57, avg(delay#70) AS avg(delay)#440]\n",
      "   +- Project [depdelay#14 AS delay#70, dest#15 AS dst#57, origin#18 AS src#44]\n",
      "      +- Filter (((isnotnull(origin#18) AND isnotnull(depdelay#14)) AND (origin#18 = ATL)) AND (depdelay#14 > 1.0))\n",
      "         +- RelationV2[depdelay#14, dest#15, origin#18] json hdfs://training.io:8020/user/training/data/flightsdata.json\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [avg(delay)#440 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(avg(delay)#440 DESC NULLS LAST, 200), true, [id=#234]\n",
      "   +- *(2) HashAggregate(keys=[src#44, dst#57], functions=[avg(delay#70)], output=[src#44, dst#57, avg(delay)#440])\n",
      "      +- Exchange hashpartitioning(src#44, dst#57, 200), true, [id=#230]\n",
      "         +- *(1) HashAggregate(keys=[src#44, dst#57], functions=[partial_avg(delay#70)], output=[src#44, dst#57, sum#449, count#450L])\n",
      "            +- *(1) Project [depdelay#14 AS delay#70, dest#15 AS dst#57, origin#18 AS src#44]\n",
      "               +- *(1) Filter (((isnotnull(origin#18) AND isnotnull(depdelay#14)) AND (origin#18 = ATL)) AND (depdelay#14 > 1.0))\n",
      "                  +- BatchScan[depdelay#14, dest#15, origin#18] JsonScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/flightsdata.json], ReadSchema: struct<depdelay:double,dest:string,origin:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.filter(\"src = 'ATL' and delay > 1\").groupBy(\"src\", \"dst\").avg(\"delay\").sort(desc(\"avg(delay)\")).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHFJaai6VbNv"
   },
   "source": [
    "## End of Exercise"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "08-SparkSQL-Partitioning-Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
