{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfyEG_F3XjOJ"
   },
   "source": [
    "# Working with File Partitioning\n",
    "\n",
    "In this exercise, we will cover How to partition the data for fast querying.\n",
    "\n",
    "In this lesson you:\n",
    " - Partition your data for increased query performance\n",
    " - Minimize the small file problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxNXLzGqXjOZ"
   },
   "source": [
    "Let's start with some CSV data in a single folder\n",
    "* people-10m.csv\n",
    "* people-10m-partitioned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hssmd2hIXjOb"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/people-10m\", header=\"true\", inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pvKqrSQRXjOc",
    "outputId": "473efff4-e029-40fb-edb3-0274ee9dccae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "+---+----------+----------+-------------+------+-------------------+-----------+------+\n",
      "| id| firstName|middleName|     lastName|gender|          birthDate|        ssn|salary|\n",
      "+---+----------+----------+-------------+------+-------------------+-----------+------+\n",
      "|  1|    Pennie|     Carry|   Hirschmann|     F|1955-07-02 04:00:00|981-43-9345| 56172|\n",
      "|  2|        An|     Amira|       Cowper|     F|1992-02-08 05:00:00|978-97-8086| 40203|\n",
      "|  3|     Quyen|    Marlen|         Dome|     F|1970-10-11 04:00:00|957-57-8246| 53417|\n",
      "|  4|   Coralie|  Antonina|      Marshal|     F|1990-04-11 04:00:00|963-39-4885| 94727|\n",
      "|  5|    Terrie|      Wava|        Bonar|     F|1980-01-16 05:00:00|964-49-8051| 79908|\n",
      "|  6|  Chassidy|Concepcion|Bourthouloume|     F|1990-11-24 05:00:00|954-59-9172| 64652|\n",
      "|  7|      Geri|    Tambra|        Mosby|     F|1970-12-19 05:00:00|968-16-4020| 38195|\n",
      "|  8|    Patria|     Nancy|      Arstall|     F|1985-01-02 05:00:00|984-76-3770|102053|\n",
      "|  9|    Terese|  Alfredia|       Tocque|     F|1967-11-17 05:00:00|967-48-7309| 91294|\n",
      "| 10|      Wava|   Lyndsey|      Jeandon|     F|1963-12-30 05:00:00|997-82-2946| 56521|\n",
      "| 11|    Sophie|   Emerita|        Hearn|     F|1979-09-17 04:00:00|977-66-4483| 90920|\n",
      "| 12|     Jodie|   Tabetha|      Laneham|     F|1959-01-31 05:00:00|923-24-9769| 90634|\n",
      "| 13|  Marietta|     Mandi|      Yansons|     F|1974-02-19 04:00:00|900-34-8083| 93162|\n",
      "| 14|   Caridad|     Maire|       Snelle|     F|1960-09-26 04:00:00|992-11-7062| 38859|\n",
      "| 15|   Yasmine|       Meg|    Edworthye|     F|1960-01-29 05:00:00|922-12-9862| 76220|\n",
      "| 16|      Chan|      Jani|       Hartas|     F|1986-12-05 05:00:00|995-51-3115| 75050|\n",
      "| 17|Evangeline|   Wanetta|    Casserley|     F|1961-09-29 04:00:00|926-61-3526| 62814|\n",
      "| 18|    Elnora|     Kecia|       Lipman|     F|1980-02-14 05:00:00|950-23-9739| 71350|\n",
      "| 19|    Adelle|   Kathyrn|    Grigoriev|     F|1978-11-14 05:00:00|923-23-5984| 60600|\n",
      "| 20|      Mica|    Zandra|     Challens|     F|1973-11-24 05:00:00|918-66-1232| 51071|\n",
      "+---+----------+----------+-------------+------+-------------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "8\n",
      "+-------+---------+----------+----------+------+-------------------+-----------+------+\n",
      "|     id|firstName|middleName|  lastName|gender|          birthDate|        ssn|salary|\n",
      "+-------+---------+----------+----------+------+-------------------+-----------+------+\n",
      "|7544038|     Lino| Margarito|      Poat|     M|1960-08-10 04:00:00|975-94-9336| 58779|\n",
      "|7544052|    Byron|    Steven|  Kniveton|     M|1994-09-19 04:00:00|902-78-5916| 86316|\n",
      "|7544053|   Leslie|    Hobert|  Courtois|     M|1993-01-16 05:00:00|945-53-9736| 70820|\n",
      "|7544076|  Agustin|      Neil|    Priest|     M|1979-05-15 04:00:00|954-93-5011| 46405|\n",
      "|7544080|      Dee|    Darryl|     Hughs|     M|1966-06-21 04:00:00|981-12-1106| 82023|\n",
      "|7544082|   Wilmer|   Russell|Sharphouse|     M|1989-12-20 05:00:00|937-45-4532|102132|\n",
      "|7544085|   Darryl|      Chas|  Cowderay|     M|1958-03-12 05:00:00|997-57-6150| 71057|\n",
      "|7544100|    Issac|     Tyler|     Eamer|     M|1980-08-05 04:00:00|919-30-9280| 54779|\n",
      "|7544107|  Arnulfo|     Alvin|   Hearley|     M|1978-10-08 04:00:00|983-90-7227| 51000|\n",
      "|7544129|    Royce|    Murray|   Westell|     M|1987-06-12 04:00:00|972-44-9071| 62399|\n",
      "|7544131|  Wendell|     Daren|   Clynter|     M|1958-12-06 05:00:00|953-73-3222|119033|\n",
      "|7544139|  Lorenzo| Francesco|   Perassi|     M|1990-05-12 04:00:00|911-37-9447| 89717|\n",
      "|7544148|     Karl|     Louis|    Speedy|     M|1994-11-09 05:00:00|987-46-7359| 59961|\n",
      "|7544154|  Antoine|   Freeman|Davidowsky|     M|1982-01-02 05:00:00|910-51-8117| 76216|\n",
      "|7544163|     Eric|      Reid|     Danet|     M|1958-09-13 04:00:00|965-60-1212| 71230|\n",
      "|7544168|   Jayson|   Elliott|  Goodbody|     M|1976-04-17 05:00:00|988-58-8780| 67546|\n",
      "|7544171|  Milford|     Pablo|    Gerold|     M|1990-12-31 05:00:00|906-19-1807| 68078|\n",
      "|7544183|  Bertram|    Lamont|   Peatman|     M|1983-11-06 05:00:00|992-58-3514| 30479|\n",
      "|7544184|  Earnest|      Rory| Buckenham|     M|1999-04-25 04:00:00|999-40-3478| 99722|\n",
      "|7544186|   Darell|      Phil|    Orfeur|     M|1999-06-13 04:00:00|964-87-1013| 77126|\n",
      "+-------+---------+----------+----------+------+-------------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.rdd.getNumPartitions())\n",
    "df.show()\n",
    "redf=df.repartition(8,\"id\",\"lastName\",\"gender\")\n",
    "print(redf.rdd.getNumPartitions())\n",
    "redf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrOrK8FKXjOe"
   },
   "source": [
    "What if when we filter by the year of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Sr3LpEKvXjOf",
    "outputId": "f106397c-c02d-4c12-fde9-08aed8a24718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Filter ((year(cast(birthDate#57 as date)) >= 1970) AND (year(cast(birthDate#57 as date)) <= 1980)), Statistics(sizeInBytes=713.3 MiB)\n",
      "+- RelationV2[id#52, firstName#53, middleName#54, lastName#55, gender#56, birthDate#57, ssn#58, salary#59] csv hdfs://training.io:8020/user/training/data/people-10m, Statistics(sizeInBytes=713.3 MiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#52, firstName#53, middleName#54, lastName#55, gender#56, birthDate#57, ssn#58, salary#59]\n",
      "+- *(1) Filter ((year(cast(birthDate#57 as date)) >= 1970) AND (year(cast(birthDate#57 as date)) <= 1980))\n",
      "   +- BatchScan[id#52, firstName#53, middleName#54, lastName#55, gender#56, birthDate#57, ssn#58, salary#59] CSVScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/people-10m], ReadSchema: struct<id:int,firstName:string,middleName:string,lastName:string,gender:string,birthDate:timestam...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"year(birthDate) between 1970 and 1980\").explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56eO0yF3XjOg"
   },
   "source": [
    "Why it took so much time or ***even more*** to count the filtered vs the whole dataset? Look at the query plan to understand.\n",
    "\n",
    "So let's try with a partitioned version instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o56.get.\n: java.util.NoSuchElementException: spark.driver.cores\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:2656)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:2656)\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:73)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8c809a497ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.driver.cores\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK3-3.0.0.2.99.0.0-23-1.cdh7.0.3.0-79.p0.1855087/lib/spark3/python/pyspark/sql/conf.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_NoValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK3-3.0.0.2.99.0.0-23-1.cdh7.0.3.0-79.p0.1855087/lib/spark3/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1286\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK3-3.0.0.2.99.0.0-23-1.cdh7.0.3.0-79.p0.1855087/lib/spark3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK3-3.0.0.2.99.0.0-23-1.cdh7.0.3.0-79.p0.1855087/lib/spark3/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o56.get.\n: java.util.NoSuchElementException: spark.driver.cores\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:2656)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:2656)\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:73)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.default.parallelism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mOmnZLBrXjOg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "df_by_year = spark.read.csv(\"data/people-10m-partitioned.csv\", header=\"true\", inferSchema=\"true\")\n",
    "print(df_by_year.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "RelationV2[id#454, firstName#455, middleName#456, lastName#457, gender#458, birthDate#459, ssn#460, salary#461, birthYear#462] csv hdfs://training.io:8020/user/training/data/people-10m-partitioned.csv, Statistics(sizeInBytes=713.3 MiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#454, firstName#455, middleName#456, lastName#457, gender#458, birthDate#459, ssn#460, salary#461, birthYear#462]\n",
      "+- BatchScan[id#454, firstName#455, middleName#456, lastName#457, gender#458, birthDate#459, ssn#460, salary#461, birthYear#462] CSVScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/people-10m-partitioned.csv], ReadSchema: struct<id:int,firstName:string,middleName:string,lastName:string,gender:string,birthDate:timestam...\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_by_year.explain(mode=\"cost\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "x1oNb0pSXjOh",
    "outputId": "2f1cdace-1999-42f9-fd17-c6827728c586"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2287326"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_year.where(\"birthYear between 1970 and 1980\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfTAnsnIXjOi"
   },
   "source": [
    "That's quite good, but let's examine the query plan.\n",
    "\n",
    "Why such small reads with 8 tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "v8THr15SXjOi",
    "outputId": "9b6b103e-d566-4b36-f885-635317239fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "18\n",
      "10000000\n"
     ]
    }
   ],
   "source": [
    "print(df_by_year.where(\"birthYear between 1970 and 1980\").rdd.getNumPartitions())\n",
    "print(df_by_year.rdd.getNumPartitions())\n",
    "print(df_by_year.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhkKmageXjOj"
   },
   "source": [
    "We have 8 small files per partition folder, very inefficient especially when it comes to cloud storage!\n",
    "\n",
    "**Question**: Why do we need `repartition` AND `partitionBy`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yXP6eWazXjOj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "(df_by_year.repartition(\"birthYear\")\n",
    "  .write.partitionBy(\"birthYear\")\n",
    "  .format(\"parquet\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"path\", \"people_by_year.parquet\")\n",
    "  .saveAsTable(\"people_by_year_optimized\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "tIpvvbOhXjOm",
    "outputId": "ad52db44-a775-4f98-ec3a-2b2e29932cae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(spark.read.table(\"people_by_year_optimized\").where(\"birthYear between 1970 AND 1980\").rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'textFiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2bf424d1ac74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/people-10m-partitioned.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'textFiles'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMZjT8zfXjOn"
   },
   "source": [
    "Now, we're reading in a single larger file per partition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYYzeHkQXjOn"
   },
   "source": [
    "## End of Exercise"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 05-Working with File Partitioning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "TTE 02 - Partitioning & File Layout",
  "notebookId": 2472
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
