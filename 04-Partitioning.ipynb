{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbeRPStFUN2T"
   },
   "source": [
    "# Partitioning\n",
    "\n",
    "**Data Source**\n",
    "* English Wikipedia pageviews by second\n",
    "* Size on Disk: ~255 MB\n",
    "* Type: Tab Separated Text File & Parquet files\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* To understand the relationship between Slots/cores and partitions\n",
    "* To review `repartition(n)` and `coalesce(n)`\n",
    "* To review one key side effect of shuffle partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i9pWAXXUN2X"
   },
   "source": [
    "### **The Data Source**\n",
    "\n",
    "For this exercise we will use the **Pageviews By Seconds** data set.\n",
    "\n",
    "The file is located on the HDFS at **data/pageviews_by_second.tsv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1614951534625,
     "user": {
      "displayName": "Harshit Gupta",
      "photoUrl": "",
      "userId": "02259099889324824661"
     },
     "user_tz": -330
    },
    "id": "uwgOEA7ilqo0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jjjdBqSMUN2Y"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType(\n",
    "  [\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"site\", StringType(), False),\n",
    "    StructField(\"requests\", IntegerType(), False)\n",
    "  ]\n",
    ")\n",
    "\n",
    "fileName = \"data/pageviews_by_second.tsv\"\n",
    "\n",
    "# Create our initial DataFrame\n",
    "initialDF = (spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .schema(schema)\n",
    "  .csv(fileName)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHfh89E1UN2f"
   },
   "source": [
    "Now let's see what our data consists of:\n",
    "\n",
    "* timestamp of the record when it was created\n",
    "* the site i.e mobile or desktop\n",
    "* Lastly the number of requests\n",
    "\n",
    "This is for the one record per site, per second, and captures the no. of requests which are made in that one second. \n",
    "\n",
    "i.e. for every second of the day, there are 2 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hoj8A6tzUN2g",
    "outputId": "ef44a059-8409-46f2-a91b-0d1baf01531b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+\n",
      "|          timestamp|   site|requests|\n",
      "+-------------------+-------+--------+\n",
      "|2015-03-16T00:09:55| mobile|    1595|\n",
      "|2015-03-16T00:10:39| mobile|    1544|\n",
      "|2015-03-16T00:19:39|desktop|    2460|\n",
      "|2015-03-16T00:38:11|desktop|    2237|\n",
      "|2015-03-16T00:42:40| mobile|    1656|\n",
      "|2015-03-16T00:52:24|desktop|    2452|\n",
      "|2015-03-16T00:54:16| mobile|    1654|\n",
      "|2015-03-16T01:18:11| mobile|    1720|\n",
      "|2015-03-16T01:30:32|desktop|    2288|\n",
      "|2015-03-16T01:32:24| mobile|    1609|\n",
      "|2015-03-16T01:42:08|desktop|    2341|\n",
      "|2015-03-16T01:45:53| mobile|    1704|\n",
      "|2015-03-16T01:55:37|desktop|    2554|\n",
      "|2015-03-16T01:57:29| mobile|    1825|\n",
      "|2015-03-16T02:03:16|desktop|    2492|\n",
      "|2015-03-16T02:10:32| mobile|    1667|\n",
      "|2015-03-16T02:16:45|desktop|    2452|\n",
      "|2015-03-16T02:19:32|desktop|    2412|\n",
      "|2015-03-16T02:20:16|desktop|    2350|\n",
      "|2015-03-16T02:22:08| mobile|    1802|\n",
      "+-------------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initialDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8N3wgxtUN2m"
   },
   "source": [
    "## Initial Steps\n",
    "\n",
    "Before processing any data, we must follow several steps to simply prepare the data for analysis:\n",
    "1. **Read the data**\n",
    "2. Balance the number of partitions with the number of slots\n",
    "3. Cache the data\n",
    "4. Adjust 'spark.sql.shuffle.partitions'\n",
    "5. Perform some of the basic ETL\n",
    "6. If the ETL was costly, re-cache the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU7KOFrjUN2o"
   },
   "source": [
    "## Slots vs Partitions\n",
    "\n",
    "* We have our 'initialDF' (**Step #1**) which tells to nothing more than reading in and through the data.\n",
    "* In our **Step #2** we must ask the question, what is the relationship between slots and partitions?\n",
    "\n",
    "\n",
    "** *Note:* ** *The Spark API uses the term **core** meaning a thread available for parallel execution.*<br/>*Here we refer to it as **slot** to avoid confusion with the number of cores in the underlying CPU(s)*<br/>*to which there isn't necessarily an equal number.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0OzFhDXUN2p"
   },
   "source": [
    "### Slots(Cores)\n",
    "\n",
    "Generally, you should know how many cores you have, when you created your cluster.\n",
    "\n",
    "For checking programatically, one can use 'SparkContext.defaultParallelism'\n",
    "\n",
    "> For operations such as parallelize with no parent RDDs, it depends on the cluster manager:\n",
    "> * Local mode: number of cores (local machine)\n",
    "> * Mesos fine grained mode: 2\n",
    "> * **Others: total number of cores on all executor nodes or 2, whichever is larger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "K6DU0Zt8UN2r",
    "outputId": "86c6073b-6935-4d6b-e12a-99a8fc7b29ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 2 cores, or slots.\n"
     ]
    }
   ],
   "source": [
    "cores = sc.defaultParallelism\n",
    "\n",
    "print(\"You have {} cores, or slots.\".format(cores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALl37Bh_UN2x"
   },
   "source": [
    "### Partitions\n",
    "\n",
    "* The 2nd Half of this question asked above is How many partitions of data do I have?\n",
    "* With these we have 2 sub-questions:\n",
    "  1. Why there are soo many?\n",
    "  2. What is a partition?\n",
    "\n",
    ">Answer to the last question is, a **partition** is a small piece of the total data/dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35-hGiNZUN2y"
   },
   "source": [
    ">Back to the 1st question, answer for it can be provided by running the following command which\n",
    ">>* acutally takes the `initialDF`\n",
    ">>* can converts it to an `RDD`\n",
    ">>* and then asks the `RDD` for the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J7rYDm8cUN2z",
    "outputId": "c3396225-6146-4a40-be5b-1de1d72cac26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "partitions = initialDF.rdd.getNumPartitions()\n",
    "print(\"Partitions: {0:,}\".format( partitions ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5QCzzNjUN24"
   },
   "source": [
    "* In Spark 3.0 there are a lot of optimizations have been added to the readers.\n",
    "* Namely the few, you can looks at **the number of slots**, the **size of the data**, and makes a optimised guess that at how many partitions **should be created**.\n",
    "\n",
    "But 2 partitions and 2 slots is just too easy.\n",
    "  * Let's read in another copy of this same data.\n",
    "  * A parquet file that was saved in 5 partitions.\n",
    "  * This gives us an excuse to reason about the **relationship between slots and partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MpCRh61QUN25",
    "outputId": "c7018646-ad7b-4e37-dad1-7761733abbd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions 5\n"
     ]
    }
   ],
   "source": [
    "# Create our initial DataFrame. We can let it infer the \n",
    "# schema because the cost for parquet files is really low.\n",
    "alternateRDD = sc.textFile(\"data/pageviews_by_second_csv\")\n",
    "\n",
    "print(\"Partitions\", alternateRDD.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh499Y59UN29"
   },
   "source": [
    "Now that we have 5 partitions we must ask:\n",
    "\n",
    "> What is going to happen, When we perform and action like `count()` **with 2 slots and only 5 partitions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w7thFAgIUN2_",
    "outputId": "3dea415f-d3f7-481d-abeb-95522d0302ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternateRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "P7-r8_dBUN3D"
   },
   "outputs": [],
   "source": [
    "alternateDF = alternateRDD.map(lambda x: x.split(\",\")).toDF([\"timestamp\",\"site\",\"request\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SbZqsh3UN3J"
   },
   "source": [
    "### Use Every Slot or Core\n",
    "\n",
    "With some very less exceptions, one always want the no. of partitions to be **a factor of the number of slots**.\n",
    "\n",
    "In such way **every slot is used**.\n",
    "\n",
    "I.e., every slots are being assigned a task.\n",
    "\n",
    "With 5 partitions & 2 slots we are **over-utilizing three slots as we have 2 slots**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyjPE4YEUN3L"
   },
   "source": [
    "### More or Less Partitions?\n",
    "\n",
    "As in **general guideline**, it is advised that each partition (when cached) is roughly around 200MB.\n",
    "* Size on disk is not a good gauge. For example:\n",
    ">* CSV files are generally large on disk but small in RAM - consider the string \"56789\" which is 10 bytes compared to the integer 56789 which is only 4 bytes.\n",
    ">* Parquet files are highly compressed but uncompressed in RAM.\n",
    "\n",
    "**Question:** If I read in data and it comes in with 10 partitions should I:\n",
    "* reduce the partitions down to 8 (1x number of slots)\n",
    "* or increase the partitions up to 16 (2x number of slots)\n",
    "\n",
    "**Answer:** It totally depends on the size of each partition\n",
    "* Firstly, Read the data. \n",
    "* then Cache it. \n",
    "* Look at the size per partition.\n",
    "* For Example, If your partition size is near or over 200MB consider increasing the number of partitions.\n",
    "* For Example, If your partition size is under 200MB consider decreasing the number of partitions.\n",
    "\n",
    "The goal will **ALWAYS** be to use as few partitions as possible while maintaining at least 1 x number-of-slots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKB2KR4-UN3M"
   },
   "source": [
    "## repartition(n) or coalesce(n)\n",
    "\n",
    "We have two operations that can help to address above problem: `repartition(n)` and `coalesce(n)`.\n",
    "\n",
    "If you look at the API docs, `coalesce(n)` is described like this:\n",
    "> Returns a new Dataset that has exactly numPartitions partitions, when fewer partitions are requested.<br/>\n",
    "> If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "If you look at the API docs, `repartition(n)` is described like this:\n",
    "> Returns a new Dataset that has exactly numPartitions partitions.\n",
    "\n",
    "The key differences between the two are\n",
    "* `coalesce(n)` is a **narrow** transformation and can only be used to reduce the number of partitions.\n",
    "* `repartition(n)` is a **wide** transformation and can be used to reduce or increase the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2FWC_1-JUN3N",
    "outputId": "fab4cf61-3f89-4c61-a699-22ae167b3407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 8\n"
     ]
    }
   ],
   "source": [
    "repartitionedDF = alternateDF.repartition(8)\n",
    "\n",
    "print(\"Partitions: {0:,}\".format( repartitionedDF.rdd.getNumPartitions() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bFbXqVgUN3S"
   },
   "source": [
    "## Want to Cache, Again?\n",
    "\n",
    "Back to general list\n",
    "1. **Read the data**\n",
    "2. **Balance the number of partitions with the number of slots**\n",
    "3. Cache the data\n",
    "4. Adjust `spark.sql.shuffle.partitions`\n",
    "5. Perform some of the basic ETL\n",
    "6. If the ETL was costly, Possibly re-cache the data\n",
    "\n",
    "We just balanced the number of partitions w.r.t. the number of slots.\n",
    "\n",
    "Depending on the no. of the partitions and the size of data, the shuffle operation can be quite expensive.\n",
    "\n",
    "Now Let's cache the result of the `repartition(n)` call\n",
    "* Or more specifically, let's mark it for caching.\n",
    "* The actual cache will occur later once an action is performed\n",
    "* Or you could just execute a count to force materialization of the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tYjiz8oiUN3U",
    "outputId": "e9e56e85-4318-4b73-fb59-6060744f1132"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: string, site: string, request: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartitionedDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p46VIGHDUN3X"
   },
   "source": [
    "## spark.sql.shuffle.partitions\n",
    "\n",
    "0. <div style=\"text-decoration:line-through\">Read the data</div>\n",
    "0. <div style=\"text-decoration:line-through\">Balance the number of partitions with the number of slots</div>\n",
    "0. <div style=\"text-decoration:line-through\">Cache the data</div>\n",
    "0. Adjust `spark.sql.shuffle.partitions`\n",
    "0. Perform some basic ETL\n",
    "0. If the ETL was costly, Possibly re-cache the data\n",
    "\n",
    "The next problem has to do with a side effect of certain **wide** transformations.\n",
    "\n",
    "So far, we haven't run any **wide** transformations other than `repartition(n)`\n",
    "* But eventually we will in thi section\n",
    "* Let's illustrate the problem that we will **eventually** hit\n",
    "* We can do this by simply sorting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TCjkEBnlUN3Z"
   },
   "outputs": [],
   "source": [
    "(repartitionedDF\n",
    "  .orderBy(col(\"timestamp\"), col(\"site\")) # sort the data\n",
    "   .foreach(lambda x: None)               # litterally does nothing except trigger a job\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUZspCNEUN3d"
   },
   "source": [
    "### Quick Detour\n",
    "\n",
    "Something is not right here\n",
    "* We only executed one action.\n",
    "* But two jobs were triggered.\n",
    "* If we look at the physical plan we can see the reason for the extra job.\n",
    "* The answer lies in the step **Exchange rangepartitioning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "j2h10gy1UN3e",
    "outputId": "343e7286-0408-43c0-a076-eed3719bcd0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [timestamp#56 ASC NULLS FIRST, site#57 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(timestamp#56 ASC NULLS FIRST, site#57 ASC NULLS FIRST, 200), true, [id=#77]\n",
      "   +- InMemoryTableScan [timestamp#56, site#57, request#58]\n",
      "         +- InMemoryRelation [timestamp#56, site#57, request#58], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               +- Exchange RoundRobinPartitioning(8), false, [id=#57]\n",
      "                  +- *(1) Scan ExistingRDD[timestamp#56,site#57,request#58]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=3000000, orderBy=[timestamp#56 ASC NULLS FIRST,site#57 ASC NULLS FIRST], output=[timestamp#56,site#57,request#58])\n",
      "+- InMemoryTableScan [timestamp#56, site#57, request#58]\n",
      "      +- InMemoryRelation [timestamp#56, site#57, request#58], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- Exchange RoundRobinPartitioning(8), false, [id=#57]\n",
      "               +- *(1) Scan ExistingRDD[timestamp#56,site#57,request#58]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "(repartitionedDF\n",
    "  .orderBy(col(\"timestamp\"), col(\"site\"))\n",
    "  .explain()\n",
    ")\n",
    "print(\"-\"*80)\n",
    "\n",
    "(repartitionedDF\n",
    "  .orderBy(col(\"timestamp\"), col(\"site\"))\n",
    "  .limit(3000000)\n",
    "  .explain()\n",
    ")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnTSmcvYUN3j"
   },
   "source": [
    "And just to prove that the extra job is due to the number of records in our DataFrame, re-run it with only 3M records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VYRKIVXOUN3k"
   },
   "outputs": [],
   "source": [
    "(repartitionedDF\n",
    "  .orderBy(col(\"timestamp\"), col(\"site\")) # sort the data\n",
    "  .limit(30000)                         # only 3 million please    \n",
    "  .foreach(lambda x: None)                # litterally does nothing except trigger a job\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa63gUjpUN3o"
   },
   "source": [
    "Only 1 job.\n",
    "\n",
    "Spark's Catalyst Optimizer is optimizing our jobs for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y4sbmyqUN3q"
   },
   "source": [
    "### The Real Problem\n",
    "\n",
    "Back to the original issue:\n",
    "* Re-run the original job.\n",
    "* Take a look at the second job.\n",
    "* Look at the 3rd Stage.\n",
    "* Notice that it has 200 partitions!\n",
    "* And this is our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dDEd18F5UN3r"
   },
   "outputs": [],
   "source": [
    "funkyDF = (repartitionedDF\n",
    "  .orderBy(col(\"timestamp\"), col(\"site\")) # sorts the data\n",
    ")                                         #\n",
    "funkyDF.foreach(lambda x: None)           # litterally does nothing except trigger a job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17-MEAnOUN3w"
   },
   "source": [
    "The problem is the number of partitions we ended up with.\n",
    "\n",
    "Besides looking at the number of tasks in the final stage, we can simply print out the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "W3gXWAGsUN3x",
    "outputId": "de960bb7-5f49-480d-fcda-dacc4afdd859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 200\n"
     ]
    }
   ],
   "source": [
    "print(\"Partitions: {0:,}\".format( funkyDF.rdd.getNumPartitions() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTbUpADFUN32"
   },
   "source": [
    "The engineers building Apache Spark chose a default value, 200, for the new partition size. After all our work to determine the right number of partitions they go and undo it on us.\n",
    "\n",
    "The value 200 is actually based on practical experience, attempting to account for the most common scenarios to date. Work is being done to intelligently determine this new value but that is still in progress.\n",
    "\n",
    "For now, we can tweak it with the configuration value `spark.sql.shuffle.partitions`\n",
    "\n",
    "We can see below that it is actually configured for 200 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8CgYFV0oUN34",
    "outputId": "3056ef57-4156-49f3-a84a-1ac80f4aed7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6ibrrkmUN3-"
   },
   "source": [
    "We can change the config setting with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "c9zsovAbUN3_"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYREg-KVUN4D"
   },
   "source": [
    "Now, if we re-run our query, we will see that we end up with the 8 partitions we want post-shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OmNJyMCoUN4E",
    "outputId": "d8aa0412-a610-4309-abce-11b3e95f8074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 8\n"
     ]
    }
   ],
   "source": [
    "betterDF = (repartitionedDF\n",
    "  .orderBy(col(\"timestamp\"), col(\"site\")) # sort the data\n",
    ")                                         #\n",
    "betterDF.foreach(lambda x: None)          # litterally does nothing except trigger a job\n",
    "\n",
    "print(\"Partitions: {0:,}\".format( betterDF.rdd.getNumPartitions() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FH3gDdKUN4G"
   },
   "source": [
    "## Initial ETL\n",
    "\n",
    "0. <div style=\"text-decoration:line-through\">Read the data</div>\n",
    "0. <div style=\"text-decoration:line-through\">Balance the number of partitions with the number of slots</div>\n",
    "0. <div style=\"text-decoration:line-through\">Cache the data</div>\n",
    "0. <div style=\"text-decoration:line-through\">Adjust `spark.sql.shuffle.partitions`</div>\n",
    "0. Perform some basic ETL\n",
    "0. If the ETL was costly, Possibly re-cache the data\n",
    "\n",
    "We may have some standard ETL.\n",
    "\n",
    "In this case we will want to do something like convert the `timestamp` column from a **string** to a data type more appropriate for **date & time**.\n",
    "\n",
    "We are not going to do that here, instead will will cover that specific case in a future notebook when we look at all the date & time functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "w7WwRORFUN4H",
    "outputId": "71e173c5-94ff-4f6f-f08a-cb195a89e047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****BEFORE****\n",
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      " |-- request: string (nullable = true)\n",
      "\n",
      "****AFTER****\n",
      "root\n",
      " |-- createdAt: timestamp (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      " |-- request: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pageviewsDF = (repartitionedDF\n",
    "  .select(\n",
    "    unix_timestamp( col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\").alias(\"createdAt\"), \n",
    "    col(\"site\"), \n",
    "    col(\"request\") \n",
    "  )\n",
    ")\n",
    "\n",
    "print(\"****BEFORE****\")\n",
    "repartitionedDF.printSchema()\n",
    "\n",
    "print(\"****AFTER****\")\n",
    "pageviewsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PCiBJD0UN4K"
   },
   "source": [
    "And assuming that initial ETL was expensive... we would want to finish up by caching our final `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uJYnao6-UN4K",
    "outputId": "814e1eb9-cd85-4b12-ddf2-ff094f46a55b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mark it as cached.\n",
    "pageviewsDF.cache() \n",
    "\n",
    "# materialize the cache.\n",
    "pageviewsDF.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXQ9YT3RUN4M"
   },
   "source": [
    "## All Done\n",
    "\n",
    "0. <div style=\"text-decoration:line-through\">Read the data</div>\n",
    "0. <div style=\"text-decoration:line-through\">Balance the number of partitions to the number of slots</div>\n",
    "0. <div style=\"text-decoration:line-through\">Cache the data</div>\n",
    "0. <div style=\"text-decoration:line-through\">Adjust `spark.sql.shuffle.partitions`</div>\n",
    "0. <div style=\"text-decoration:line-through\">Perform some basic ETL</div>\n",
    "0. <div style=\"text-decoration:line-through\">If the ETL was costly, Possibly re-cache the data</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 04-Partitioning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "Partitioning",
  "notebookId": 9689
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
