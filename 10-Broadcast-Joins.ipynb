{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9uS9SeUW6q6"
   },
   "source": [
    "# Broadcast Joins\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* Introduce the concept of Broadcast Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYsnwVUxu8uw"
   },
   "source": [
    "In this exercise you will:<br>\r\n",
    "Learn about the concepts of Broadcast Joins\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwT0sf8bW6rE"
   },
   "source": [
    "## The Data Source\n",
    "\n",
    "This data uses the **Pageviews By Seconds** data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bI-Ka6esW6rF"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# I've already gone through the exercise to determine\n",
    "# how many partitions I want and in this case it is...\n",
    "partitions = 8\n",
    "\n",
    "# Make sure wide operations don't repartition to 200\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QCpAxAo4W6rG"
   },
   "outputs": [],
   "source": [
    "# The directory containing our parquet files.\n",
    "parquetFile = \"data/pageviews_by_second.parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Gv4yEyZFW6rG",
    "outputId": "3f60d13f-2488-4b9a-a2a4-8e966fd4ca86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our initial DataFrame. We can let it infer the \n",
    "# schema because the cost for parquet files is really low.\n",
    "pageviewsDF = (spark.read\n",
    "  .option(\"inferSchema\", \"true\")                # The default, but not costly w/Parquet\n",
    "  .parquet(parquetFile)                         # Read the data in\n",
    "  .repartition(partitions)                      # From 7 >>> 8 partitions\n",
    "  .withColumnRenamed(\"timestamp\", \"capturedAt\") # rename and convert to timestamp datatype\n",
    "  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n",
    "  .orderBy( col(\"capturedAt\"), col(\"site\") )    # sort our records\n",
    "  .cache()                                      # Cache the expensive operation\n",
    ")\n",
    "# materialize the cache\n",
    "pageviewsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJJMhGVrW6rI"
   },
   "source": [
    "## Broadcast Joins\n",
    "\n",
    "If you saw the section on UDFs carefully, you can say that we can **aggregate by the Day-Of-Week**.\n",
    "\n",
    "We will **first use a UDF** only to discover that either there was a built in function to do the exact same thing or not.\n",
    "\n",
    "We also saw that **Monday had more data** than that of any other day of the week.\n",
    "\n",
    "We will then forked the `DataFrame` in such a way so as it can be compared with the **Mobile Requests to Desktop Requests**.\n",
    "\n",
    "Next, we will **join those to `DataFrames`** into one, so that we could easily compare the two sets of the data.\n",
    "\n",
    "We you can say that the problem with the data **has nothing to do with Mobile vs Desktop**.\n",
    "\n",
    "So we don't need that type of join (two ~large `DataFrames`)\n",
    "\n",
    "However, what if we wanted to **reproduce our first exercise** (counts per day-of-week)\n",
    "* without a UDF\n",
    "* with a lookup table for the day-of-week\n",
    "* with a join between the pageviews and the lookup table\n",
    "\n",
    "What's different about this example is that we are **joining a big `DataFrame` to a small `DataFrame`**.\n",
    "\n",
    "In this scenario, Spark can optimize the join and **avoid the expensive shuffle** with a **Broadcast Join**.\n",
    "\n",
    "Let's start with two `DataFrames`\n",
    "* The first we will derive from our original `DataFrame`. In this case, we will use a simple number for the day-of-week.\n",
    "* The second `DataFrame` will map that number (1-7) to the labels **Monday**, **Tue**, **W**, or whatever...\n",
    "\n",
    "Let's take a look at our first `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_NZRjVEnW6rJ",
    "outputId": "399c5845-bbc2-4d47-c48d-ca045987466c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+----+\n",
      "|         capturedAt|   site|requests| dow|\n",
      "+-------------------+-------+--------+----+\n",
      "|2015-03-16 00:00:00|desktop|    2343|2015|\n",
      "|2015-03-16 00:00:00| mobile|    1628|2015|\n",
      "|2015-03-16 00:00:01|desktop|    2382|2015|\n",
      "|2015-03-16 00:00:01| mobile|    1636|2015|\n",
      "|2015-03-16 00:00:02|desktop|    2546|2015|\n",
      "|2015-03-16 00:00:02| mobile|    1619|2015|\n",
      "|2015-03-16 00:00:03|desktop|    2402|2015|\n",
      "|2015-03-16 00:00:03| mobile|    1776|2015|\n",
      "|2015-03-16 00:00:04|desktop|    2370|2015|\n",
      "|2015-03-16 00:00:04| mobile|    1716|2015|\n",
      "|2015-03-16 00:00:05|desktop|    2417|2015|\n",
      "|2015-03-16 00:00:05| mobile|    1721|2015|\n",
      "|2015-03-16 00:00:06|desktop|    2318|2015|\n",
      "|2015-03-16 00:00:06| mobile|    1695|2015|\n",
      "|2015-03-16 00:00:07|desktop|    2580|2015|\n",
      "|2015-03-16 00:00:07| mobile|    1630|2015|\n",
      "|2015-03-16 00:00:08|desktop|    2545|2015|\n",
      "|2015-03-16 00:00:08| mobile|    1731|2015|\n",
      "|2015-03-16 00:00:09|desktop|    2366|2015|\n",
      "|2015-03-16 00:00:09| mobile|    1664|2015|\n",
      "+-------------------+-------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnTrans = date_format(col(\"capturedAt\"), \"u\").alias(\"dow\")\n",
    "\n",
    "pageviewsWithDowDF = (pageviewsDF\n",
    "    .withColumn(\"dow\", columnTrans)  # Add the column dow\n",
    ")\n",
    "(pageviewsWithDowDF\n",
    "  .cache()                           # mark the data as cached\n",
    "  .count()                           # materialize the cache\n",
    ")\n",
    "pageviewsWithDowDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8U1fOw8W6rJ"
   },
   "source": [
    "All we did here is that, add one column **dow** which has the value **1** for **Monday**, **2** for **Tuesday**, etc.\n",
    "\n",
    "Next, we are going to load a mapping of 1, 2, 3, etc. to Mon, Tue, Wed, etc from a **REALLY** small `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4k8EJY2fW6rK",
    "outputId": "0b42c689-6af2-4114-e04d-6e72c33ca990"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dow: int, longName: string, abbreviated: string, shortName: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labelsDF = spark.read.parquet(\"data/day-of-week\")\n",
    "\n",
    "display(labelsDF) # view our labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIr-r9dVW6rK"
   },
   "source": [
    "Now that we have the two `DataFrames`.\n",
    "\n",
    "Finally, we can execute a join between the two `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m8cmQXtCW6rK",
    "outputId": "418f0933-d991-44fc-d222-446ac31d2c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+---+--------+-----------+---------+\n",
      "|capturedAt|site|requests|dow|longName|abbreviated|shortName|\n",
      "+----------+----+--------+---+--------+-----------+---------+\n",
      "+----------+----+--------+---+--------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDowDF = (pageviewsWithDowDF\n",
    "  .join(labelsDF, pageviewsWithDowDF[\"dow\"] == labelsDF[\"dow\"])\n",
    "  .drop( pageviewsWithDowDF[\"dow\"] )\n",
    ")\n",
    "joinedDowDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elwwG4ArW6rL"
   },
   "source": [
    "Now, that the data is joined, we can easily aggregate by any (or all) of the various labels which represents the day-of-week.\n",
    "\n",
    "Notice that we are not losing the numerical **dow** column which we can use to sort.\n",
    "\n",
    "And when we will group this, you can group by any one of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QHUmwpbVW6rL",
    "outputId": "e77151db-cd5e-40de-d455-21ecd7b58838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+---------+--------+\n",
      "|dow|longName|abbreviated|shortName|Requests|\n",
      "+---+--------+-----------+---------+--------+\n",
      "+---+--------+-----------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregatedDowDF = (joinedDowDF\n",
    "  .groupBy(col(\"dow\"), col(\"longName\"), col(\"abbreviated\"), col(\"shortName\"))  \n",
    "  .sum(\"requests\")                                             \n",
    "  .withColumnRenamed(\"sum(requests)\", \"Requests\")\n",
    "  .orderBy(col(\"dow\"))\n",
    ")\n",
    "# Display and then graph...\n",
    "aggregatedDowDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kL4e5GOJW6rM"
   },
   "source": [
    "## Already Broadcasted\n",
    "\n",
    "You beleive or not, that was the broadcast join.\n",
    "\n",
    "The proof of which can be seen by looking at the physical plan.\n",
    "\n",
    "Run the `explain()` and then look for **BroadcastHashJoin** and/or **BroadcastExchange**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gGEOvBMJW6rM",
    "outputId": "7a9d3bbd-f0f1-4912-f03d-a5a6981376f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Sort [dow#306 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(dow#306 ASC NULLS FIRST, 8), true, [id=#276]\n",
      "   +- *(3) HashAggregate(keys=[dow#306, longName#307, abbreviated#308, shortName#309], functions=[sum(cast(requests#2 as bigint))])\n",
      "      +- Exchange hashpartitioning(dow#306, longName#307, abbreviated#308, shortName#309, 8), true, [id=#272]\n",
      "         +- *(2) HashAggregate(keys=[dow#306, longName#307, abbreviated#308, shortName#309], functions=[partial_sum(cast(requests#2 as bigint))])\n",
      "            +- *(2) Project [requests#2, dow#306, longName#307, abbreviated#308, shortName#309]\n",
      "               +- *(2) BroadcastHashJoin [cast(dow#86 as int)], [dow#306], Inner, BuildRight\n",
      "                  :- *(2) Filter isnotnull(dow#86)\n",
      "                  :  +- InMemoryTableScan [requests#2, dow#86], [isnotnull(dow#86)]\n",
      "                  :        +- InMemoryRelation [capturedAt#10, site#1, requests#2, dow#86], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  :              +- *(1) Project [capturedAt#10, site#1, requests#2, date_format(capturedAt#10, u, Some(UTC)) AS dow#86]\n",
      "                  :                 +- InMemoryTableScan [capturedAt#10, requests#2, site#1]\n",
      "                  :                       +- InMemoryRelation [capturedAt#10, site#1, requests#2], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  :                             +- *(3) Sort [capturedAt#10 ASC NULLS FIRST, site#1 ASC NULLS FIRST], true, 0\n",
      "                  :                                +- Exchange rangepartitioning(capturedAt#10 ASC NULLS FIRST, site#1 ASC NULLS FIRST, 8), true, [id=#29]\n",
      "                  :                                   +- *(2) Project [cast(unix_timestamp(timestamp#0, yyyy-MM-dd'T'HH:mm:ss, Some(UTC)) as timestamp) AS capturedAt#10, site#1, requests#2]\n",
      "                  :                                      +- Exchange RoundRobinPartitioning(8), false, [id=#25]\n",
      "                  :                                         +- *(1) ColumnarToRow\n",
      "                  :                                            +- BatchScan[timestamp#0, site#1, requests#2] ParquetScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/pageviews_by_second.parquet], ReadSchema: struct<timestamp:string,site:string,requests:int>, PushedFilters: []\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#266]\n",
      "                     +- *(1) Project [dow#306, longName#307, abbreviated#308, shortName#309]\n",
      "                        +- *(1) Filter isnotnull(dow#306)\n",
      "                           +- *(1) ColumnarToRow\n",
      "                              +- BatchScan[dow#306, longName#307, abbreviated#308, shortName#309] ParquetScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/day-of-week], ReadSchema: struct<dow:int,longName:string,abbreviated:string,shortName:string>, PushedFilters: [IsNotNull(dow)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregatedDowDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKXOAyyuW6rM"
   },
   "source": [
    "From the code perspective, it just looks like other joins.\n",
    "\n",
    "So what's the difference between a regular and a broadcast-join?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voH8c7v3W6rN"
   },
   "source": [
    "## Standard Join\n",
    "\n",
    ">* In a standard join, **ALL** of the data is shuffled\n",
    ">* This can be really expensive\n",
    "<br/><br/>\n",
    "<p>Here we can see, how all the records keyed by \"green\" are moved to the same partition.<br/>The process would be repeated for \"red\" and \"blue\" records.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnVKhuAoW6rN"
   },
   "source": [
    "## Broadcast Join\n",
    ">* In the Broadcast Join, only the \"small\" data is moved.\n",
    ">* It duplicates the \"small\" data across all executors.\n",
    ">* But the \"big\" data is left untouched.\n",
    ">* If the \"small\" data is small enough, this can be **VERY** efficient.\n",
    "<br/><br/>\n",
    "<p>Here we see the records keyed by \"red\" being replicated into the first partition.<br/>\n",
    "   The process would be repeated for each executor.<br/>\n",
    "   The entire process would be repeated again for \"green\" and \"blue\" records.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWyVEswkW6rN"
   },
   "source": [
    "## Broadcasted, How?\n",
    "\n",
    "Behind the scenes, Spark is analyzing our two `DataFrames`.\n",
    "\n",
    "It attempts to estimate if either or both are < 10MB.\n",
    "\n",
    "We can see/change this threshold value with the config **spark.sql.autoBroadcastJoinThreshold**. \n",
    "\n",
    "The documentation reads as follows:\n",
    "> Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "s0Ou_2DeW6rN",
    "outputId": "eff8e8aa-a8ea-4121-8d34-93d476e148c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold:10485760b\n"
     ]
    }
   ],
   "source": [
    "threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "#print(\"Threshold: {0:,}\".format( int(threshold)))\n",
    "print(\"Threshold:\" + threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJz6F6eLW6rP"
   },
   "source": [
    "For this kind of cases, it will take the small `DataFrame`, the `labelsDF` in our case\n",
    ">* Send the entire `DataFrame` to every **Executor**\n",
    ">* Then do a join on the local copy of `labelsDF`\n",
    ">* Compared to taking our big `DataFrame` `pageviewsWithDowDF` and shuffling it across all executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-cNAfQZW6rQ"
   },
   "source": [
    "We can see proof of this by dropping the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hyzZGaZYW6rQ"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaRCdCnjW6rR"
   },
   "source": [
    "Run the `explain()` and take a look for the **ABSENCE OF** the **BroadcastHashJoin** and/or **BroadcastExchange**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IqRtZjhqW6rR",
    "outputId": "c8d343b2-3f72-4c4d-ab9a-8acdb31cb017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) Sort [dow#306 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(dow#306 ASC NULLS FIRST, 8), true, [id=#358]\n",
      "   +- *(5) HashAggregate(keys=[dow#306, longName#307, abbreviated#308, shortName#309], functions=[sum(cast(requests#2 as bigint))])\n",
      "      +- *(5) HashAggregate(keys=[dow#306, longName#307, abbreviated#308, shortName#309], functions=[partial_sum(cast(requests#2 as bigint))])\n",
      "         +- *(5) Project [requests#2, dow#306, longName#307, abbreviated#308, shortName#309]\n",
      "            +- *(5) SortMergeJoin [cast(dow#86 as int)], [dow#306], Inner\n",
      "               :- *(2) Sort [cast(dow#86 as int) ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(cast(dow#86 as int), 8), true, [id=#338]\n",
      "               :     +- *(1) Filter isnotnull(dow#86)\n",
      "               :        +- InMemoryTableScan [requests#2, dow#86], [isnotnull(dow#86)]\n",
      "               :              +- InMemoryRelation [capturedAt#10, site#1, requests#2, dow#86], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :                    +- *(1) Project [capturedAt#10, site#1, requests#2, date_format(capturedAt#10, u, Some(UTC)) AS dow#86]\n",
      "               :                       +- InMemoryTableScan [capturedAt#10, requests#2, site#1]\n",
      "               :                             +- InMemoryRelation [capturedAt#10, site#1, requests#2], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :                                   +- *(3) Sort [capturedAt#10 ASC NULLS FIRST, site#1 ASC NULLS FIRST], true, 0\n",
      "               :                                      +- Exchange rangepartitioning(capturedAt#10 ASC NULLS FIRST, site#1 ASC NULLS FIRST, 8), true, [id=#29]\n",
      "               :                                         +- *(2) Project [cast(unix_timestamp(timestamp#0, yyyy-MM-dd'T'HH:mm:ss, Some(UTC)) as timestamp) AS capturedAt#10, site#1, requests#2]\n",
      "               :                                            +- Exchange RoundRobinPartitioning(8), false, [id=#25]\n",
      "               :                                               +- *(1) ColumnarToRow\n",
      "               :                                                  +- BatchScan[timestamp#0, site#1, requests#2] ParquetScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/pageviews_by_second.parquet], ReadSchema: struct<timestamp:string,site:string,requests:int>, PushedFilters: []\n",
      "               +- *(4) Sort [dow#306 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(dow#306, 8), true, [id=#348]\n",
      "                     +- *(3) Project [dow#306, longName#307, abbreviated#308, shortName#309]\n",
      "                        +- *(3) Filter isnotnull(dow#306)\n",
      "                           +- *(3) ColumnarToRow\n",
      "                              +- BatchScan[dow#306, longName#307, abbreviated#308, shortName#309] ParquetScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/day-of-week], ReadSchema: struct<dow:int,longName:string,abbreviated:string,shortName:string>, PushedFilters: [IsNotNull(dow)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(joinedDowDF\n",
    "  .groupBy(col(\"dow\"), col(\"longName\"), col(\"abbreviated\"), col(\"shortName\"))  \n",
    "  .sum(\"requests\")                                             \n",
    "  .withColumnRenamed(\"sum(requests)\", \"Requests\")\n",
    "  .orderBy(col(\"dow\"))\n",
    "  .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4q4tpFiW6rR"
   },
   "source": [
    "And now that we are done, let's restore the original threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "m2CxRSljW6rR"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU9_EFW7W6rS"
   },
   "source": [
    "## broadcast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9QMwC9KW6rS"
   },
   "source": [
    "What if we wanted to broadcast the data and it was over the 10MB [default] threshold?\n",
    "\n",
    "Now, we can specify that a `DataFrame` is to be broadcasted by using the `broadcast()` operation from the `sql.functions` package.\n",
    "\n",
    "However, **it is only a hint**. Spark is allowed to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IEB7I7X0W6rS",
    "outputId": "2cbc50fe-5f12-4a42-9009-411eb97591be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[capturedAt: timestamp, site: string, requests: int, dow: string, dow: int, longName: string, abbreviated: string, shortName: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageviewsWithDowDF.join(broadcast(labelsDF), pageviewsWithDowDF[\"dow\"] == labelsDF[\"dow\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQXIEDnsW6rT"
   },
   "source": [
    "## End of Exercise"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 10-Broadcast-Joins.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "Intro To DF Part 5",
  "notebookId": 10796
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
