{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuMK14XJWjyj"
   },
   "source": [
    "# Catalyst Optimizer\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* Understanding about what is the Catalyst Optimizer?\n",
    "* Understanding the different stages of the Catalyst Optimizer\n",
    "* Example of Physical Plan Optimization (x2)\n",
    "* Example of Predicate Pushdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B7iBj1l1Wjyy"
   },
   "outputs": [],
   "source": [
    "# Because we will need it later...\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSB0IEQ-Wjyz"
   },
   "source": [
    "## Catalyst Optimizer\n",
    "\n",
    "* Catalyst Optimize is the fundamental to the `SQL` and `DataFrames` API.\n",
    "* It is an **extensible query optimizer**.\n",
    "* It actually contains a **general library for representing trees and applying rules** to manipulate them.\n",
    "* Several public extension points, including external data sources and user-defined types.\n",
    "\n",
    "<a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\">Deep Dive into Spark SQLâ€™s Catalyst Optimizer</a> (April 13, 2015)\n",
    "\n",
    "Processing is broken down into several stages as we can see here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqIoJhj3Wjy1"
   },
   "source": [
    "![Catalyst](https://files.training.databricks.com/images/105/catalyst-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9T-7bApWjy2"
   },
   "source": [
    "## Optimized Logical Plan\n",
    "\n",
    "**Rewriting our code** is one of the many optimizations performed by the Catalyst Optimizer.\n",
    "  \n",
    "For this, we will see **two basic examples** involving the rewriting of the filters.\n",
    "\n",
    "First one is an **innocent mistake** almost most every new Spark developer makes.\n",
    "\n",
    "The second \"mistake\" is **really bad**, but Spark can fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuL4jPu1Wjy2"
   },
   "source": [
    "### Example #1: Innocent Mistake(The First One)\n",
    "\n",
    "Don't start any project with **en.zero**.\n",
    "\n",
    "There are **always better ways of doing this**, as in it can be done with a single condition.\n",
    "\n",
    "But, here we would make **8 passes** on the data **with 8 different filters**.\n",
    "\n",
    "After that every individual pass, we will **go back over the remaining dataset** to filter out the next set of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "odaV_B-yWjy3",
    "outputId": "7785393b-63b9-4736-cc34-56153a8aaeff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 1: 2,288,138\n",
      "Pass 2: 2,288,123\n",
      "Pass 3: 2,288,071\n",
      "Pass 4: 2,287,665\n",
      "Pass 5: 2,287,641\n",
      "Pass 6: 2,287,314\n",
      "Pass 7: 2,287,271\n",
      "Pass 8: 2,287,196\n"
     ]
    }
   ],
   "source": [
    "allDF = spark.read.parquet(\"data/staging_parquet_en_only_clean/\")\n",
    "\n",
    "pass1 = allDF.filter( col(\"project\") != \"en.zero\")\n",
    "pass2 = pass1.filter( col(\"project\") != \"en.zero.n\")\n",
    "pass3 = pass2.filter( col(\"project\") != \"en.zero.s\")\n",
    "pass4 = pass3.filter( col(\"project\") != \"en.zero.d\")\n",
    "pass5 = pass4.filter( col(\"project\") != \"en.zero.voy\")\n",
    "pass6 = pass5.filter( col(\"project\") != \"en.zero.b\")\n",
    "pass7 = pass6.filter( col(\"project\") != \"en.zero.v\")\n",
    "pass8 = pass7.filter( col(\"project\") != \"en.zero.q\")\n",
    "\n",
    "print(\"Pass 1: {0:,}\".format( pass1.count() ))\n",
    "print(\"Pass 2: {0:,}\".format( pass2.count() ))\n",
    "print(\"Pass 3: {0:,}\".format( pass3.count() ))\n",
    "print(\"Pass 4: {0:,}\".format( pass4.count() ))\n",
    "print(\"Pass 5: {0:,}\".format( pass5.count() ))\n",
    "print(\"Pass 6: {0:,}\".format( pass6.count() ))\n",
    "print(\"Pass 7: {0:,}\".format( pass7.count() ))\n",
    "print(\"Pass 8: {0:,}\".format( pass8.count() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxhPtrQyWjy6"
   },
   "source": [
    "**Logically**, the code above is the same as the code below.\n",
    "\n",
    "The only real difference is that we are **not asking for a count** after every filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8IYPCQ86Wjy7",
    "outputId": "37b7341b-94f6-47d2-9bd0-4393b9039176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Count: 2,287,196\n"
     ]
    }
   ],
   "source": [
    "innocentDF = (spark.read.parquet(\"data/staging_parquet_en_only_clean/\")\n",
    "  .filter( col(\"project\") != \"en.zero\")\n",
    "  .filter( col(\"project\") != \"en.zero.n\")\n",
    "  .filter( col(\"project\") != \"en.zero.s\")\n",
    "  .filter( col(\"project\") != \"en.zero.d\")\n",
    "  .filter( col(\"project\") != \"en.zero.voy\")\n",
    "  .filter( col(\"project\") != \"en.zero.b\")\n",
    "  .filter( col(\"project\") != \"en.zero.v\")\n",
    "  .filter( col(\"project\") != \"en.zero.q\")\n",
    ")\n",
    "print(\"Final Count: {0:,}\".format( innocentDF.count() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs6oQJT5Wjy7"
   },
   "source": [
    "R=There is no need to execute the code to see what is **logically** or **physically** taking place under the hood.\n",
    "\n",
    "We can directly use the `explain(..)` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iS5DewSrWjy8",
    "outputId": "5d06e4ac-f449-4109-e439-00a611fb4ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter NOT ('project = en.zero.q)\n",
      "+- Filter NOT (project#96 = en.zero.v)\n",
      "   +- Filter NOT (project#96 = en.zero.b)\n",
      "      +- Filter NOT (project#96 = en.zero.voy)\n",
      "         +- Filter NOT (project#96 = en.zero.d)\n",
      "            +- Filter NOT (project#96 = en.zero.s)\n",
      "               +- Filter NOT (project#96 = en.zero.n)\n",
      "                  +- Filter NOT (project#96 = en.zero)\n",
      "                     +- RelationV2[project#96, article#97, requests#98, bytes_served#99L] parquet hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "project: string, article: string, requests: int, bytes_served: bigint\n",
      "Filter NOT (project#96 = en.zero.q)\n",
      "+- Filter NOT (project#96 = en.zero.v)\n",
      "   +- Filter NOT (project#96 = en.zero.b)\n",
      "      +- Filter NOT (project#96 = en.zero.voy)\n",
      "         +- Filter NOT (project#96 = en.zero.d)\n",
      "            +- Filter NOT (project#96 = en.zero.s)\n",
      "               +- Filter NOT (project#96 = en.zero.n)\n",
      "                  +- Filter NOT (project#96 = en.zero)\n",
      "                     +- RelationV2[project#96, article#97, requests#98, bytes_served#99L] parquet hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter ((((((((isnotnull(project#96) AND NOT (project#96 = en.zero)) AND NOT (project#96 = en.zero.n)) AND NOT (project#96 = en.zero.s)) AND NOT (project#96 = en.zero.d)) AND NOT (project#96 = en.zero.voy)) AND NOT (project#96 = en.zero.b)) AND NOT (project#96 = en.zero.v)) AND NOT (project#96 = en.zero.q))\n",
      "+- RelationV2[project#96, article#97, requests#98, bytes_served#99L] parquet hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [project#96, article#97, requests#98, bytes_served#99L]\n",
      "+- *(1) Filter ((((((((isnotnull(project#96) AND NOT (project#96 = en.zero)) AND NOT (project#96 = en.zero.n)) AND NOT (project#96 = en.zero.s)) AND NOT (project#96 = en.zero.d)) AND NOT (project#96 = en.zero.voy)) AND NOT (project#96 = en.zero.b)) AND NOT (project#96 = en.zero.v)) AND NOT (project#96 = en.zero.q))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- BatchScan[project#96, article#97, requests#98, bytes_served#99L] ParquetScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean], ReadSchema: struct<project:string,article:string,requests:int,bytes_served:bigint>, PushedFilters: [IsNotNull(project), Not(EqualTo(project,en.zero)), Not(EqualTo(project,en.zero.n)), Not(EqualTo(project,en.zero.s)), Not(EqualTo(project,en.zero.d)), Not(EqualTo(project,en.zero.voy)), Not(EqualTo(project,en.zero.b)), Not(EqualTo(project,en.zero.v)), Not(EqualTo(project,en.zero.q))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "innocentDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXEmulRyWjy8"
   },
   "source": [
    "Of course, if we were to write this the correct way, the first time, ignoring the fact that there are better methods, it would look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G5sEExgmWjy9",
    "outputId": "36b30b92-6665-4de5-f651-9aad356ef9a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: 2,287,196\n",
      "== Parsed Logical Plan ==\n",
      "'Filter ((((((((isnotnull('project) AND NOT ('project = en.zero)) AND NOT ('project = en.zero.n)) AND NOT ('project = en.zero.s)) AND NOT ('project = en.zero.d)) AND NOT ('project = en.zero.voy)) AND NOT ('project = en.zero.b)) AND NOT ('project = en.zero.v)) AND NOT ('project = en.zero.q))\n",
      "+- RelationV2[project#118, article#119, requests#120, bytes_served#121L] parquet hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "project: string, article: string, requests: int, bytes_served: bigint\n",
      "Filter ((((((((isnotnull(project#118) AND NOT (project#118 = en.zero)) AND NOT (project#118 = en.zero.n)) AND NOT (project#118 = en.zero.s)) AND NOT (project#118 = en.zero.d)) AND NOT (project#118 = en.zero.voy)) AND NOT (project#118 = en.zero.b)) AND NOT (project#118 = en.zero.v)) AND NOT (project#118 = en.zero.q))\n",
      "+- RelationV2[project#118, article#119, requests#120, bytes_served#121L] parquet hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter ((((((((isnotnull(project#118) AND NOT (project#118 = en.zero)) AND NOT (project#118 = en.zero.n)) AND NOT (project#118 = en.zero.s)) AND NOT (project#118 = en.zero.d)) AND NOT (project#118 = en.zero.voy)) AND NOT (project#118 = en.zero.b)) AND NOT (project#118 = en.zero.v)) AND NOT (project#118 = en.zero.q))\n",
      "+- RelationV2[project#118, article#119, requests#120, bytes_served#121L] parquet hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [project#118, article#119, requests#120, bytes_served#121L]\n",
      "+- *(1) Filter ((((((((isnotnull(project#118) AND NOT (project#118 = en.zero)) AND NOT (project#118 = en.zero.n)) AND NOT (project#118 = en.zero.s)) AND NOT (project#118 = en.zero.d)) AND NOT (project#118 = en.zero.voy)) AND NOT (project#118 = en.zero.b)) AND NOT (project#118 = en.zero.v)) AND NOT (project#118 = en.zero.q))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- BatchScan[project#118, article#119, requests#120, bytes_served#121L] ParquetScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean], ReadSchema: struct<project:string,article:string,requests:int,bytes_served:bigint>, PushedFilters: [IsNotNull(project), Not(EqualTo(project,en.zero)), Not(EqualTo(project,en.zero.n)), Not(EqualTo(project,en.zero.s)), Not(EqualTo(project,en.zero.d)), Not(EqualTo(project,en.zero.voy)), Not(EqualTo(project,en.zero.b)), Not(EqualTo(project,en.zero.v)), Not(EqualTo(project,en.zero.q))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "betterDF = (spark.read.parquet(\"data/staging_parquet_en_only_clean/\")\n",
    "  .filter( (col(\"project\").isNotNull()) &\n",
    "           (col(\"project\") != \"en.zero\") & \n",
    "           (col(\"project\") != \"en.zero.n\") & \n",
    "           (col(\"project\") != \"en.zero.s\") & \n",
    "           (col(\"project\") != \"en.zero.d\") & \n",
    "           (col(\"project\") != \"en.zero.voy\") & \n",
    "           (col(\"project\") != \"en.zero.b\") & \n",
    "           (col(\"project\") != \"en.zero.v\") & \n",
    "           (col(\"project\") != \"en.zero.q\")\n",
    "        )\n",
    ")\n",
    "\n",
    "print(\"Final: {0:,}\".format( betterDF.count() ))\n",
    "\n",
    "betterDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-LXjqHwWjy-"
   },
   "source": [
    "### Example #2: Bad Programmer(The Second One)\n",
    "\n",
    "This time we intendedly are going to do something **REALLY** bad.\n",
    "\n",
    "Even if the compiler combines these filters into a single filter, **we still have five different tests** for any column that doesn't have the value \"whatever\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8SkB6ef7Wjy-",
    "outputId": "3e773c4c-8079-4dbe-aaa5-59a4046e8f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter NOT ('project = whatever)\n",
      "+- Filter NOT (project#140 = whatever)\n",
      "   +- Filter NOT (project#140 = whatever)\n",
      "      +- Filter NOT (project#140 = whatever)\n",
      "         +- Filter NOT (project#140 = whatever)\n",
      "            +- RelationV2[project#140, article#141, requests#142, bytes_served#143L] parquet hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "project: string, article: string, requests: int, bytes_served: bigint\n",
      "Filter NOT (project#140 = whatever)\n",
      "+- Filter NOT (project#140 = whatever)\n",
      "   +- Filter NOT (project#140 = whatever)\n",
      "      +- Filter NOT (project#140 = whatever)\n",
      "         +- Filter NOT (project#140 = whatever)\n",
      "            +- RelationV2[project#140, article#141, requests#142, bytes_served#143L] parquet hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(project#140) AND NOT (project#140 = whatever))\n",
      "+- RelationV2[project#140, article#141, requests#142, bytes_served#143L] parquet hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [project#140, article#141, requests#142, bytes_served#143L]\n",
      "+- *(1) Filter (isnotnull(project#140) AND NOT (project#140 = whatever))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- BatchScan[project#140, article#141, requests#142, bytes_served#143L] ParquetScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/staging_parquet_en_only_clean], ReadSchema: struct<project:string,article:string,requests:int,bytes_served:bigint>, PushedFilters: [IsNotNull(project), Not(EqualTo(project,whatever))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stupidDF = (spark.read.parquet(\"data/staging_parquet_en_only_clean/\")\n",
    "  .filter( col(\"project\") != \"whatever\")\n",
    "  .filter( col(\"project\") != \"whatever\")\n",
    "  .filter( col(\"project\") != \"whatever\")\n",
    "  .filter( col(\"project\") != \"whatever\")\n",
    "  .filter( col(\"project\") != \"whatever\")\n",
    ")\n",
    "\n",
    "stupidDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6t88vNg4Wjy_"
   },
   "source": [
    "***Note:*** `explain()` is not the only way to get access to this level of detail.<br/>\n",
    "But, we can also see it in the **Spark UI**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sfu9I72TWjy_"
   },
   "source": [
    "## Columnar Predicate Pushdown\n",
    "\n",
    "It takes place when a filter can be pushed down to the original data source, such as the database server.\n",
    "\n",
    "For this example, we are going to compare `DataFrames` from two different sources:\n",
    "* JDBC - where a predicate pushdown **WILL** take place.\n",
    "* CSV - where a predicate pushdown will **NOT** take place.\n",
    "\n",
    "In each case, we can see evidence of the pushdown (or lack of it) in the **Physical Plan**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZsfFIa0WjzA"
   },
   "source": [
    "### Example #3: JDBC\n",
    "\n",
    "We will start by initializing the JDBC driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbgWd_feWjzB"
   },
   "source": [
    "Next, we can create a `DataFrame` via JDBC and then filter by **gender**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AtNFMNEoWjzC"
   },
   "outputs": [],
   "source": [
    "jdbcURL = \"jdbc:mysql://localhost/retail_db\"\n",
    "\n",
    "# Username and Password w/read-only rights\n",
    "connProperties = {\n",
    "  \"user\" : \"root\",\n",
    "  \"password\" : \"password\",\n",
    "  \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "ppExampleThreeDF = (spark.read.jdbc(\n",
    "    url=jdbcURL,                  # the JDBC URL\n",
    "    table=\"orders\",   # the name of the table\n",
    "    column=\"order_id\",                  # the name of a column of an integral type that will be used for partitioning\n",
    "    lowerBound=1,                 # the minimum value of columnName used to decide partition stride\n",
    "    upperBound=1000000,           # the maximum value of columnName used to decide partition stride\n",
    "    numPartitions=8,              # the number of partitions/connections\n",
    "    properties=connProperties     # the connection properties\n",
    "  )\n",
    "  .filter(col(\"order_status\") == \"COMPLETE\")   # Filter the data by gender\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp6tdaHQWjzC"
   },
   "source": [
    "With the `DataFrame` that is created, we can ask Spark to `explain()` the **Physical Plan**.\n",
    "\n",
    "What we are looking for in this Physical Plan:\n",
    "* is the lack of a **Filter** and\n",
    "* the presence of a **PushedFilters** in the **Scan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "p3Ik6DGDWjzD",
    "outputId": "9af82d71-f0b9-4c60-de1b-8deb7f41b80a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(orders) [numPartitions=8] [order_id#152,order_date#153,order_customer_id#154,order_status#155] PushedFilters: [*IsNotNull(order_status), *EqualTo(order_status,COMPLETE)], ReadSchema: struct<order_id:int,order_date:timestamp,order_customer_id:int,order_status:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ppExampleThreeDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JSEOy6WWjzD"
   },
   "source": [
    "This will make a little more sense if we **compare it to some examples** that don't push down the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuLNEEbLWjzE"
   },
   "source": [
    "### Example #4: Cached JDBC\n",
    "\n",
    "In this example, we are going to cache our data before filtering and thus eliminating the possibility for the predicate push down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CvVSU0MWWjzE"
   },
   "outputs": [],
   "source": [
    "ppExampleFourCachedDF = (spark.read.jdbc(\n",
    "    url=jdbcURL,                  # the JDBC URL\n",
    "    table=\"orders\",   # the name of the table\n",
    "    column=\"order_id\",                  # the name of a column of an integral type that will be used for partitioning\n",
    "    lowerBound=1,                 # the minimum value of columnName used to decide partition stride\n",
    "    upperBound=1000000,           # the maximum value of columnName used to decide partition stride\n",
    "    numPartitions=8,              # the number of partitions/connections\n",
    "    properties=connProperties     # the connection properties\n",
    "  ))\n",
    "\n",
    "(ppExampleFourCachedDF\n",
    "  .cache()                        # cache the data\n",
    "  .count())                       # materialize the cache\n",
    "\n",
    "ppExampleFourFilteredDF = (ppExampleFourCachedDF\n",
    "  .filter(col(\"order_status\") == \"COMPLETE\"))  # Filter the data by gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZanBEPmNWjzF"
   },
   "source": [
    "Now that we have cached the data and Then filtered it. We have to eliminate the possibility to bennifet from the predicate push down.\n",
    "\n",
    "And so that it's easier to compare the two examples, we can re-print the physical plan for the previous example too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3bmIJ7xmWjzF",
    "outputId": "768d92de-8702-4829-f741-b823e0621c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Example Three****\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(orders) [numPartitions=8] [order_id#152,order_date#153,order_customer_id#154,order_status#155] PushedFilters: [*IsNotNull(order_status), *EqualTo(order_status,COMPLETE)], ReadSchema: struct<order_id:int,order_date:timestamp,order_customer_id:int,order_status:string>\n",
      "\n",
      "\n",
      "\n",
      "****Example Four****\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(order_status#163) AND (order_status#163 = COMPLETE))\n",
      "+- InMemoryTableScan [order_id#160, order_date#161, order_customer_id#162, order_status#163], [isnotnull(order_status#163), (order_status#163 = COMPLETE)]\n",
      "      +- InMemoryRelation [order_id#160, order_date#161, order_customer_id#162, order_status#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(1) Scan JDBCRelation(orders) [numPartitions=8] [order_id#160,order_date#161,order_customer_id#162,order_status#163] PushedFilters: [], ReadSchema: struct<order_id:int,order_date:timestamp,order_customer_id:int,order_status:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"****Example Three****\\n\")\n",
    "ppExampleThreeDF.explain()\n",
    "\n",
    "print(\"\\n****Example Four****\\n\")\n",
    "ppExampleFourFilteredDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2ZsYjdyWjzG"
   },
   "source": [
    "It should be clearer now...\n",
    "\n",
    "In the first example we see only the **Scan** which is the JDBC read.\n",
    "\n",
    "In the second example, you can see the **Scan** but you also see the **InMemoryTableScan** followed by a **Filter** which means Spark had to filter ALL the data from RAM instead of in the Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auG2BhTSWjzG"
   },
   "source": [
    "### Example #5: CSV File\n",
    "\n",
    "This example is identical to the previous one, but only changes are:\n",
    "* this is a CSV file instead of JDBC source\n",
    "* we are filtering on **site**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KZ_x5NH6WjzH"
   },
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "  [\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"site\", StringType(), False),\n",
    "    StructField(\"requests\", IntegerType(), False)\n",
    "  ]\n",
    ")\n",
    "\n",
    "ppExampleThreeDF = (spark.read\n",
    "   .option(\"header\", \"true\")\n",
    "   .option(\"sep\", \"\\t\")\n",
    "   .schema(schema)\n",
    "   .csv(\"data/pageviews_by_second.tsv\")\n",
    "   .filter(col(\"site\") == \"desktop\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqXY8sCHWjzI"
   },
   "source": [
    "Once the `DataFrame` created, we can ask Spark to `explain(..)` the **Physical Plan**.\n",
    "\n",
    "What we are looking in Physical Plan:\n",
    "* is the presence of a **Filter** and\n",
    "* is the presence of a **PushedFilters** in the **FileScan csv**\n",
    "\n",
    "And again, we see **PushedFilters** because Spark is *trying* to push down to the CSV file.\n",
    "\n",
    "But that doesn't work here and so we see that just like in the last example, we have a **Filter** after the **FileScan**, actually an **InMemoryFileIndex**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2Y8n2Fm8WjzJ",
    "outputId": "0c74782c-690d-4255-e8b5-f22539e5b023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [timestamp#317, site#318, requests#319]\n",
      "+- *(1) Filter (isnotnull(site#318) AND (site#318 = desktop))\n",
      "   +- BatchScan[timestamp#317, site#318, requests#319] CSVScan Location: InMemoryFileIndex[hdfs://training.io:8020/user/training/data/pageviews_by_second.tsv], ReadSchema: struct<timestamp:string,site:string,requests:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ppExampleThreeDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQWjN4dVWjzJ"
   },
   "source": [
    "## End of Exercise"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "09-Catalyst-Optimizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "Catalyst Optimizer",
  "notebookId": 9651
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
